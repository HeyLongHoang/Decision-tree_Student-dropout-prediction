{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load student data from csv file and remove any entries that has `Target` = \"Enrolled\" to avoid noise in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_FIELDS = {\n",
    "    'Marital status',\n",
    "    'Application mode',\n",
    "    'Application order',\n",
    "    'Course',\n",
    "    'Daytime/evening attendance\\t',\n",
    "    'Previous qualification',\n",
    "    'Previous qualification (grade)',\n",
    "    'Nacionality',\n",
    "    \"Mother's qualification\",\n",
    "    \"Father's qualification\",\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    'Admission grade',\n",
    "    'Displaced',\n",
    "    'Educational special needs',\n",
    "    'Debtor',\n",
    "    'Tuition fees up to date',\n",
    "    'Gender',\n",
    "    'Scholarship holder',\n",
    "    'Age at enrollment',\n",
    "    'International',\n",
    "    'Curricular units 1st sem (credited)',\n",
    "    'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)',\n",
    "    'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate',\n",
    "    'Inflation rate',\n",
    "    'GDP',\n",
    "}\n",
    "\n",
    "def load_student_data(path_data='data/student-dropout/data.csv', feature_fields=FEATURE_FIELDS):\n",
    "    \"\"\"\n",
    "    Load dataset and remove 'Enrolled' target\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    with open(path_data, encoding='utf-8-sig') as f_data:\n",
    "        for datum in csv.DictReader(f_data, delimiter=';'):\n",
    "            remove = False\n",
    "\n",
    "            for field in list(datum.keys()):\n",
    "                if field in feature_fields and datum[field]:\n",
    "                    datum[field] = float(datum[field])\n",
    "                if field == 'Target':\n",
    "                    if datum[field] == 'Enrolled':\n",
    "                        remove = True\n",
    "                    elif datum[field] == 'Dropout':\n",
    "                        datum[field] = -1.\n",
    "                    else: # 'Graduated'\n",
    "                        datum[field] = 1.\n",
    "            \n",
    "            if not remove:\n",
    "                data.append(datum)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = load_student_data()\n",
    "\n",
    "print('Loaded', len(student_data), 'student records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some machine learning techniques to prepare data for training models, which may help improve the performance of the algorithm:\n",
    "\n",
    "- `std_vals` and `standard` work together to standardize a given feature in a dataset to have mean 0 and standard deviation 1.\n",
    "- `raw` returns the input value as is, without any transformation.\n",
    "- `one_hot` is used to handle categorical features by creating binary vectors for each unique category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_vals(data, f):\n",
    "    \"\"\" Compute average and standard deviation of feature f\"\"\"\n",
    "\n",
    "    vals = [entry[f] for entry in data]\n",
    "    avg = sum(vals)/len(vals)\n",
    "    dev = [(entry[f] - avg)**2 for entry in data]\n",
    "    sd = (sum(dev)/len(vals))**0.5\n",
    "\n",
    "    return (avg, sd)\n",
    "\n",
    "def standard(v, std):\n",
    "    \"\"\" Standardize a value v given the average and standard deviation \"\"\"\n",
    "\n",
    "    return [(v-std[0])/std[1]]\n",
    "\n",
    "def raw(x):\n",
    "    \"\"\" No transformation \"\"\"\n",
    "\n",
    "    return [x]\n",
    "\n",
    "def one_hot(v, entries):\n",
    "    \"\"\" One-hot encoding \"\"\"\n",
    "\n",
    "    vec = len(entries)*[0]\n",
    "    vec[entries.index(v)] = 1\n",
    "    return vec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply feature transformation techniques to the data to prepare it for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transforms = [\n",
    "    ('Marital status', one_hot),\n",
    "    ('Application mode', one_hot),\n",
    "    ('Application order', raw),\n",
    "    ('Course', one_hot),\n",
    "    ('Daytime/evening attendance\\t', raw),\n",
    "    ('Previous qualification', one_hot),\n",
    "    ('Previous qualification (grade)', standard),\n",
    "    ('Nacionality', one_hot),\n",
    "    (\"Mother's qualification\", one_hot),\n",
    "    (\"Father's qualification\", one_hot),\n",
    "    (\"Mother's occupation\", one_hot),\n",
    "    (\"Father's occupation\", one_hot),\n",
    "    ('Admission grade', standard),\n",
    "    ('Displaced', raw),\n",
    "    ('Educational special needs', raw),\n",
    "    ('Debtor', raw),\n",
    "    ('Tuition fees up to date', raw),\n",
    "    ('Gender', raw),\n",
    "    ('Scholarship holder', raw),\n",
    "    ('Age at enrollment', raw),\n",
    "    ('International', raw),\n",
    "    ('Curricular units 1st sem (credited)', raw),\n",
    "    ('Curricular units 1st sem (enrolled)', raw),\n",
    "    ('Curricular units 1st sem (evaluations)', raw),\n",
    "    ('Curricular units 1st sem (approved)', raw),\n",
    "    ('Curricular units 1st sem (grade)', raw),\n",
    "    ('Curricular units 1st sem (without evaluations)', raw),\n",
    "    ('Curricular units 2nd sem (credited)', raw),\n",
    "    ('Curricular units 2nd sem (enrolled)', raw),\n",
    "    ('Curricular units 2nd sem (evaluations)', raw),\n",
    "    ('Curricular units 2nd sem (approved)', raw),\n",
    "    ('Curricular units 2nd sem (grade)', raw),\n",
    "    ('Curricular units 2nd sem (without evaluations)', raw),\n",
    "    ('Unemployment rate', standard),\n",
    "    ('Inflation rate', standard),\n",
    "    ('GDP', standard),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, features, verbose=True):\n",
    "    \"\"\" Preprocess data according to features \"\"\"\n",
    "\n",
    "    features = [('Target', raw)] + features\n",
    "    std = {f:std_vals(data, f) \\\n",
    "           for (f, phi) in features if phi==standard}\n",
    "    entries = {f:list(set([entry[f] for entry in data])) \\\n",
    "               for (f, phi) in features if phi==one_hot}\n",
    "    if verbose:\n",
    "        print('Mean and Std:', std)\n",
    "        print('Entries in one_hot field:', entries)\n",
    "    \n",
    "    findex = 0\n",
    "    # Print the meaning of features\n",
    "    for (f, phi) in features[1:]: # skip 'Target'\n",
    "        if phi == standard:\n",
    "            if verbose: print(findex, f, 'std')\n",
    "            findex += 1\n",
    "        elif phi == one_hot:\n",
    "            for entry in entries[f]:\n",
    "                if verbose: print(findex, f, entry, 'one_hot')\n",
    "                findex += 1\n",
    "        else:\n",
    "            if verbose: print(findex, f, 'raw')\n",
    "            findex += 1\n",
    "\n",
    "    # Transform data\n",
    "    vals = []\n",
    "    for entry in data:\n",
    "        phis = []\n",
    "        for (f, phi) in features:\n",
    "            if phi == standard:\n",
    "                phis.extend(phi(entry[f], std[f]))\n",
    "            elif phi == one_hot:\n",
    "                phis.extend(phi(entry[f], entries[f]))\n",
    "            else:\n",
    "                phis.extend(phi(entry[f]))\n",
    "        vals.append(np.array([phis])) # phis of shape (1,D)\n",
    "\n",
    "    data_labels = np.vstack(vals)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data_labels)\n",
    "    return data_labels[:, 1:], data_labels[:, 0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(student_data, feature_transforms, verbose=False)\n",
    "print('\\nData shape:', X.shape)\n",
    "print('Labels shape:', y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `random_split` and `stratified_split` functions are used to split a dataset into training and testing sets.\n",
    "\n",
    "- The `random_split` function randomly splits the dataset into training and testing sets, where the proportion of data in the testing set is determined by the `test_pct` parameter.\n",
    "\n",
    "- The `stratified_split` function performs a similar split, but ensures that the distribution of the target variable is maintained in both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, labels, test_pct=0.2, seed=None):\n",
    "    \"\"\" Randomly split data and labels into train and test sets \"\"\"\n",
    "\n",
    "    if seed and isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    n, d = data.shape\n",
    "    idxs = np.random.permutation(n)\n",
    "    split_pt = int((1 - test_pct) * n)\n",
    "    train_idxs, test_idxs = idxs[:split_pt], idxs[split_pt:]\n",
    "    X_train, y_train = data[train_idxs, :], labels[train_idxs, :]\n",
    "    X_test, y_test = data[test_idxs, :], labels[test_idxs, :]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, labels, test_pct=0.2, seed=None):\n",
    "    \"\"\" Split data and labels into train and test sets, preserving label ratios \"\"\"\n",
    "\n",
    "    if seed and isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    test_label_counts = (label_counts * test_pct).astype(int)\n",
    "    train_label_counts = label_counts - test_label_counts\n",
    "    train_idxs, test_idxs = [], []\n",
    "\n",
    "    for label, train_count, test_count in zip(unique_labels, train_label_counts, test_label_counts):\n",
    "        label_idxs = np.where(labels == label)[0] # return an array of indexes\n",
    "        permuted_idxs = np.random.permutation(label_idxs)\n",
    "        train_idxs.extend(permuted_idxs[:train_count])\n",
    "        test_idxs.extend(permuted_idxs[train_count:])\n",
    "\n",
    "    X_train, y_train = data[train_idxs], labels[train_idxs]\n",
    "    X_test, y_test = data[test_idxs], labels[test_idxs]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stratified_split` or `random_split` function is used to split the dataset into training and testing sets while maintaining the distribution of the target variable in both sets. \n",
    "\n",
    "We then visualize the distribution of the target variable. The resulting plots show the distribution of the target variable in both sets, highlighting any differences or imbalances between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = random_split(X, y)\n",
    "X_train, y_train, X_test, y_test = stratified_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts(y):\n",
    "    vals, cnts = np.unique(y, return_counts=True)\n",
    "    probs = cnts / np.sum(cnts)\n",
    "    for value, count, prob in zip(vals, cnts, probs):\n",
    "        print(f\"  {value} appears {count} times - {prob*100:.2f}%\")\n",
    "    return vals, cnts, probs\n",
    "\n",
    "print('Train labels:')\n",
    "train_vals, train_cnts, train_probs = value_counts(y_train)\n",
    "print('Test labels:')\n",
    "test_vals, test_cnts, test_probs = value_counts(y_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "ax1.bar(train_vals, train_probs)\n",
    "ax1.set_title(\"Training set\")\n",
    "ax1.set_xlabel(\"Unique value\")\n",
    "ax1.set_ylabel(\"Probability\")\n",
    "\n",
    "ax2.bar(test_vals, test_probs)\n",
    "ax2.set_title(\"Testing set\")\n",
    "ax2.set_xlabel(\"Unique value\")\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accuracy`, `precision`, `recall`, and `f1` functions are used to evaluate the performance of a classification model. These metrics are calculated based on the predicted and true labels of a set of data points. \n",
    "\n",
    "We also applied Laplace smoothing to avoid division by zero errors and to ensure that each class has a non-zero score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "# Calculate precision\n",
    "def precision(y_pred, y_true):\n",
    "    class_labels = np.unique(y_true)  # Get unique class labels\n",
    "    k = len(class_labels)\n",
    "    precision_scores = []\n",
    "    for label in class_labels:\n",
    "        TP = np.sum(np.logical_and(y_pred == label, y_true == label))\n",
    "        FP = np.sum(np.logical_and(y_pred == label, y_true != label))\n",
    "        precision = (TP + 1e-8) / (TP + FP + 1e-8 * k)\n",
    "        precision_scores.append(precision)\n",
    "    average_precision = np.mean(precision_scores)\n",
    "    return average_precision\n",
    "\n",
    "# Calculate recall\n",
    "def recall(y_pred, y_true):\n",
    "    class_labels = np.unique(y_true)  # Get unique class labels\n",
    "    k = len(class_labels)\n",
    "    recall_scores = []\n",
    "    for label in class_labels:\n",
    "        TP = np.sum(np.logical_and(y_pred == label, y_true == label))\n",
    "        FN = np.sum(np.logical_and(y_pred != label, y_true == label))\n",
    "        recall = (TP + 1e-8) / (TP + FN + 1e-8 * k)\n",
    "        recall_scores.append(recall)\n",
    "    average_recall = np.mean(recall_scores)\n",
    "    return average_recall\n",
    "\n",
    "# Calculate F1 score\n",
    "def f1(y_pred, y_true):\n",
    "    class_labels = np.unique(y_true)  # Get unique class labels\n",
    "    k = len(class_labels)\n",
    "    f1_scores = []\n",
    "    for label in class_labels:\n",
    "        TP = np.sum(np.logical_and(y_pred == label, y_true == label))\n",
    "        FP = np.sum(np.logical_and(y_pred == label, y_true != label))\n",
    "        FN = np.sum(np.logical_and(y_pred != label, y_true == label))\n",
    "        precision = (TP + 1e-8) / (TP + FP + 1e-8 * k)\n",
    "        recall = (TP + 1e-8) / (TP + FN + 1e-8 * k)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    average_f1 = np.mean(f1_scores)\n",
    "    return average_f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementations of three machine learning algorithms for classification: `decision trees`, `random forests`, and `bagging`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    N_THRESHOLD = 4                 # don't split if node has fewer examples than this\n",
    "    H_THRESHOLD = .01               # don't split if node has entropy less than this\n",
    "    H_REDUCTION_THRESHOLD = .001    # don't split if it doesn't reduce H by this\n",
    "    MAX_DEPTH = 10                  # maximum depth of the tree\n",
    "\n",
    "    index = 0\n",
    "    tree_depth = 0                  # current depth of the whole tree\n",
    "\n",
    "    def __init__(self, data=None, config=None, depth=0):\n",
    "        \"\"\" Create a node in the decision tree. \"\"\"\n",
    "        \n",
    "        self.config = config\n",
    "        if config != None:\n",
    "            self.N_THRESHOLD = config[0]\n",
    "            self.H_THRESHOLD = config[1]\n",
    "            self.H_REDUCTION_THRESHOLD = config[2]\n",
    "            self.MAX_DEPTH = config[3]\n",
    "\n",
    "        DTNode.index += 1\n",
    "        self.index = DTNode.index           # unique number for each node\n",
    "        self.data = data                    # data at the node\n",
    "        self.prob = None                    # probability of positive label (label = 1) for the data at the node\n",
    "        self.depth = depth                  # depth of the node in the tree\n",
    "        if self.depth > DTNode.tree_depth:\n",
    "            DTNode.tree_depth = self.depth\n",
    "\n",
    "        if data is not None:\n",
    "            self.n = float(data.shape[0])          # number of data points at the node\n",
    "            self.indices = range(data.shape[1]-1)  # indices of features (used for splitting)\n",
    "            self.set_h()                           # compute entropy of the node\n",
    "\n",
    "        self.splits = {}    # splits for each feature (key = feature index, value = C{List} of thresholds)\n",
    "        self.fi = None      # feature index of the best split\n",
    "        self.th = None      # threshold of the best split\n",
    "        self.lc = None      # left child node\n",
    "        self.rc = None      # right child node\n",
    "        self.parent = None  # parent node\n",
    "\n",
    "    def set_h(self):\n",
    "        \"\"\" Set the entropy of the node, assumes labels are 1, -1 \"\"\"\n",
    "\n",
    "        b = .001\n",
    "        npos = np.sum(self.data[:, -1] == 1)  # count labels = 1\n",
    "        prob = (npos + b) / (self.n + b + b)\n",
    "        self.prob = prob\n",
    "        self.h = -prob*np.log2(prob) - (1-prob)*np.log2(1-prob)\n",
    "\n",
    "    def build_tree(self):\n",
    "        \"\"\" Build the tree recursively \"\"\"\n",
    "\n",
    "        # don't split if entropy is low, or if there are few examples, or if we're too deep\n",
    "        if self.h < self.H_THRESHOLD or self.n < self.N_THRESHOLD or self.depth >= self.MAX_DEPTH:\n",
    "            return\n",
    "\n",
    "        # find best split (with max information gain)\n",
    "        (i, th, (h, lc, rc)) = self.argmax([(i, th, self.split_eval(i, th)) \\\n",
    "                                            for i in self.indices            \\\n",
    "                                                for th in self.get_splits(i)], \n",
    "                                           lambda x: self.h - x[2][0])\n",
    "\n",
    "        # don't split if it doesn't reduce entropy enough\n",
    "        if self.h - h < self.H_REDUCTION_THRESHOLD:\n",
    "            return\n",
    "        \n",
    "        self.fi = i\n",
    "        self.th = th\n",
    "        self.lc = lc\n",
    "        self.rc = rc\n",
    "        lc.parent = self\n",
    "        rc.parent = self\n",
    "        \n",
    "        # recursively build children\n",
    "        lc.build_tree()\n",
    "        rc.build_tree()\n",
    "\n",
    "    def get_splits(self, i):\n",
    "        \"\"\" Find the best splitting point for data at node along feature at index i \"\"\"\n",
    "\n",
    "        if i not in self.splits:\n",
    "            self.splits[i] = np.sort(np.unique(self.data[:,i]), axis=None)\n",
    "        return self.splits[i]\n",
    "\n",
    "    def split_eval(self, i, th):\n",
    "        \"\"\" Evaluate weighted average entropy of splitting feature at index i by threshold th \"\"\"\n",
    "\n",
    "        lc = DTNode(self.data[self.data[:, i] < th], config=self.config, depth=self.depth+1)\n",
    "        rc = DTNode(self.data[self.data[:, i] >= th], config=self.config, depth=self.depth+1)\n",
    "        pl = lc.n / self.n\n",
    "        pr = 1.0 - pl\n",
    "        avgH = pl*lc.h + pr*rc.h\n",
    "\n",
    "        return avgH, lc, rc\n",
    "    \n",
    "    def classify(self, x):\n",
    "        \"\"\" Classify a single example \"\"\"\n",
    "\n",
    "        # return probability of positive label if leaf node\n",
    "        if self.fi == None:\n",
    "            return self.prob            \n",
    "        if x[self.fi] < self.th:\n",
    "            return self.lc.classify(x)\n",
    "        else:\n",
    "            return self.rc.classify(x)\n",
    "\n",
    "    def display(self, depth=0, max_display_depth=3):\n",
    "        \"\"\" Display the tree \"\"\"\n",
    "\n",
    "        if depth > max_display_depth:\n",
    "            print(depth*'  ', 'Depth >', max_display_depth)\n",
    "            return\n",
    "        if self.fi is None:\n",
    "            print('\\033[32m', end=\"\")\n",
    "            print(depth*'  ', f'[L={depth}]', \"%.2f\"%self.prob, '[', 'n=', \"%.0f\"%self.n, ']')\n",
    "            print('\\033[0m', end=\"\")            \n",
    "            return\n",
    "        print(depth*'  ', f'[N={depth}]', 'fi', self.fi, '< th=', \"%.5f\"%self.th, '[', 'n=', \"%.0f\"%self.n, ']')\n",
    "        self.lc.display(depth+1, max_display_depth)\n",
    "        self.rc.display(depth+1, max_display_depth)\n",
    "\n",
    "    @staticmethod\n",
    "    def argmax(l, f):\n",
    "        \"\"\"\n",
    "        @param l: C{List} of items\n",
    "        @param f: C{Procedure} that maps an item into a numeric score\n",
    "        @returns: the element of C{l} that has the highest score\n",
    "        \"\"\"\n",
    "        vals = [f(x) for x in l]\n",
    "        return l[vals.index(max(vals))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\" Decision Tree Classifier \"\"\"\n",
    "\n",
    "    def fit(self, X, Y, config=None):\n",
    "        D = np.hstack([X,Y])\n",
    "        self.root = DTNode(D, config=config)\n",
    "        self.root.build_tree()\n",
    "        return self.root\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred = np.array([np.apply_along_axis(self.root.classify, 1, X)]).T - 0.5\n",
    "        pred[pred >= 0] = 1\n",
    "        pred[pred < 0] = -1\n",
    "        return pred\n",
    "    \n",
    "    def display(self, depth=0, max_depth=3):\n",
    "        self.root.display(depth, max_depth)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging:\n",
    "    \"\"\" Bagging Classifier \"\"\"\n",
    "    \n",
    "    def __init__(self, num_trees=5):\n",
    "        self.ntrees = num_trees\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, Y, config=None):\n",
    "        for i in range(self.ntrees):\n",
    "            idxs = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_train = X[idxs, :]\n",
    "            Y_train = Y[idxs, :]\n",
    "            dt = DecisionTree()\n",
    "            dt.fit(X_train, Y_train, config)\n",
    "            self.trees.append(dt)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        if len(self.trees) == 0: return None\n",
    "        for dt in self.trees:\n",
    "            pred = dt.predict(X)\n",
    "            preds.append(pred)\n",
    "        preds = np.hstack(preds)\n",
    "        return np.sign(preds.mean(axis=1, keepdims=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\" Random Forest Classifier \"\"\"\n",
    "    \n",
    "    def __init__(self, num_trees=5, num_features=None):\n",
    "        self.ntrees = num_trees\n",
    "        self.nfeats = num_features\n",
    "        self.trees = []\n",
    "        self.feats = []\n",
    "    \n",
    "    def fit(self, X, Y, config=None):\n",
    "        for i in range(self.ntrees):\n",
    "            idxs = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            if self.nfeats is not None:\n",
    "                features = np.random.choice(X.shape[1], size=self.nfeats, replace=False)\n",
    "                X_train = X[idxs][:, features]\n",
    "                self.feats.append(features)\n",
    "            else:\n",
    "                X_train = X[idxs]\n",
    "            Y_train = Y[idxs]\n",
    "            dt = DecisionTree()\n",
    "            dt.fit(X_train, Y_train, config)\n",
    "            self.trees.append(dt)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        if len(self.trees) == 0:\n",
    "            return None\n",
    "        for i in range(len(self.trees)):\n",
    "            dt = self.trees[i]\n",
    "            if self.nfeats is not None:\n",
    "                features = self.feats[i]\n",
    "                X_test = X[:, features]\n",
    "            else:\n",
    "                X_test = X\n",
    "                \n",
    "            pred = dt.predict(X_test)\n",
    "            preds.append(pred)\n",
    "        preds = np.hstack(preds)\n",
    "        return np.sign(np.mean(preds, axis=1, keepdims=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the training data and evaluate its performance using the validation data. We then evaluate the performance of the model on the testing data to get a final assessment of the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_class, X_train, Y_train, X_test, Y_test, max_depth=5, verbose=True, config=None, args=None):\n",
    "    if args:\n",
    "        model = model_class(*args)\n",
    "    else:\n",
    "        model = model_class()\n",
    "    model.fit(X_train, Y_train, config)\n",
    "    pred_test = model.predict(X_test)\n",
    "    acc_s = accuracy(pred_test, Y_test)\n",
    "    prec_s = precision(pred_test, Y_test)\n",
    "    rec_s = recall(pred_test, Y_test)\n",
    "    f1_s = f1(pred_test, Y_test)\n",
    "    if verbose:\n",
    "        print(f'  Accuracy  : {acc_s:.5f}')\n",
    "        print(f'  Precision : {prec_s:.5f}')\n",
    "        print(f'  Recall    : {rec_s:.5f}')\n",
    "        print(f'  F1        : {f1_s:.5f}')\n",
    "        if isinstance(model, DecisionTree): \n",
    "            model.display(max_depth=max_depth)\n",
    "    return acc_s, prec_s, rec_s, f1_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, labels, k=10, model='DecisionTree', verbose=True, config=None, args=None):\n",
    "    if model == 'Bagging':\n",
    "        model_class = Bagging\n",
    "    elif model == 'RandomForest':\n",
    "        model_class = RandomForest\n",
    "    else:\n",
    "        model_class = DecisionTree\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    X = data[indices,:]\n",
    "    Y = labels[indices,:]\n",
    "    s_data = np.array_split(X, k, axis=0)\n",
    "    s_labels = np.array_split(Y, k, axis=0)\n",
    "    acc_s = 0.\n",
    "    prec_s = 0.\n",
    "    rec_s = 0.\n",
    "    f1_s = 0.\n",
    "    for i in range(k):\n",
    "        X_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=0)\n",
    "        Y_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=0)\n",
    "        X_test = np.array(s_data[i])\n",
    "        Y_test = np.array(s_labels[i])\n",
    "        if verbose == \"first_tree\":\n",
    "            if i == 0:\n",
    "                acc, prec, rec, f1 = evaluate(model_class, X_train, Y_train, X_test, Y_test, verbose=True, config=config, args=args)\n",
    "            else:\n",
    "                acc, prec, rec, f1 = evaluate(model_class, X_train, Y_train, X_test, Y_test, verbose=False, config=config, args=args)\n",
    "        elif verbose == True:\n",
    "            print('Round',i+1)\n",
    "            acc, prec, rec, f1 = evaluate(model_class, X_train, Y_train, X_test, Y_test, verbose=True, config=config, args=args)\n",
    "        else:\n",
    "            acc, prec, rec, f1 = evaluate(model_class, X_train, Y_train, X_test, Y_test, verbose=False, config=config, args=args)\n",
    "        acc_s += acc\n",
    "        prec_s += prec\n",
    "        rec_s += rec\n",
    "        f1_s += f1\n",
    "    print('\\nCROSS VALIDATION RESULTS:')\n",
    "    print(f'- Avg. Accuracy  : {acc_s/k:.5f}')\n",
    "    print(f'- Avg. Precision : {prec_s/k:.5f}')\n",
    "    print(f'- Avg. Recall    : {rec_s/k:.5f}')\n",
    "    print(f'- Avg. F1        : {f1_s/k:.5f}')\n",
    "    return acc_s/k, prec_s/k, rec_s/k, f1_s/k\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, the model is fitted using the training data (`X_train` and `y_train`). After that, we evaluate the performance of the model using the validation data (`X_val` and `y_val`) and employ cross-validation to fine-tune hyperparameters. \n",
    "\n",
    "This method helps to estimate how well the model will perform on new, unseen data, and enables us to make any necessary adjustments to the model. By using the validation dataset, we can also avoid overfitting and ensure that the model generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, y_dev, X_val, y_val = stratified_split(X_train, y_train)\n",
    "\n",
    "evaluate(DecisionTree, X_dev, y_dev, X_val, y_val, config=[4, .01, .001, 4], verbose=False)\n",
    "# evaluate(RandomForest, X_dev, y_dev, X_val, y_val, args=[5, 2], config=[4, .01, .001, 7])\n",
    "# evaluate(Bagging, X_dev, y_dev, X_val, y_val, args=[5], config=[4, .01, .001, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(X_train, y_train, k=5, verbose=True, config=[4, .01, .001, 3])\n",
    "cross_validate(X_train, y_train, model='RandomForest', k=5, verbose=True, args=[10, 200])\n",
    "# cross_validate(X_train, y_train, model='Bagging', k=5, verbose=True, args=[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the final performance of the trained and validated model on a separate testing dataset (`X_test` and `y_test`) to provide a final assessment of the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(RandomForest, X_train, y_train, X_test, y_test, args=[10, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(RandomForest, X_train, y_train, X_test, y_test, args=[5, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 11)\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "\n",
    "for max_d in max_depths:\n",
    "    acc, prec, rec, f1_ = evaluate(DecisionTree, X_dev, y_dev, X_val, y_val, config=[4, .01, .001, max_d], verbose=False)\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1_)\n",
    "    print(f'Max depth {max_d} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8))\n",
    "fig.suptitle(\"Decision Tree (varying max depths)\")\n",
    "\n",
    "ax1.plot(max_depths, accs)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Max depth\")\n",
    "\n",
    "ax2.plot(max_depths, precs)\n",
    "ax2.set_title(\"Precision\")\n",
    "ax2.set_xlabel(\"Max depth\")\n",
    "\n",
    "ax3.plot(max_depths, recs)\n",
    "ax3.set_title(\"Recall\")\n",
    "ax3.set_xlabel(\"Max depth\")\n",
    "\n",
    "ax4.plot(max_depths, f1s)\n",
    "ax4.set_title(\"F1 score\")\n",
    "ax4.set_xlabel(\"Max depth\")\n",
    "\n",
    "plt.tight_layout()  # Optional: Adjust the spacing between subplots\n",
    "\n",
    "plt.savefig('images/dt_max_depths.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying the number of features with the number of trees fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats_list = np.unique(np.logspace(np.log10(1), np.log10(238), num=20, dtype=int))\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "\n",
    "for n_feats in n_feats_list:\n",
    "    acc, prec, rec, f1_ = evaluate(RandomForest, X_dev, y_dev, X_val, y_val, args=[5, n_feats], verbose=False)\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8))\n",
    "fig.suptitle(\"Random Forest (varying n. features)\")\n",
    "\n",
    "ax1.plot(n_feats_list, accs)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Number of features\")\n",
    "\n",
    "ax2.plot(n_feats_list, precs)\n",
    "ax2.set_title(\"Precision\")\n",
    "ax2.set_xlabel(\"Number of features\")\n",
    "\n",
    "ax3.plot(n_feats_list, recs)\n",
    "ax3.set_title(\"Recall\")\n",
    "ax3.set_xlabel(\"Number of features\")\n",
    "\n",
    "ax4.plot(n_feats_list, f1s)\n",
    "ax4.set_title(\"F1 score\")\n",
    "ax4.set_xlabel(\"Number of features\")\n",
    "\n",
    "plt.tight_layout()  # Optional: Adjust the spacing between subplots\n",
    "\n",
    "# plt.savefig('images/rf_n_feats.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying the number of trees while the number of features fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees_list = np.unique(np.logspace(np.log10(1), np.log10(30), num=10, dtype=int))\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    acc, prec, rec, f1_ = evaluate(RandomForest, X_dev, y_dev, X_val, y_val, args=[n_trees, 150], verbose=False)\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1_)\n",
    "    print(f'N. trees: {n_trees} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8))\n",
    "fig.suptitle(\"Random Forest (varying n. trees)\")\n",
    "\n",
    "ax1.plot(n_trees_list, accs)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"N. trees\")\n",
    "\n",
    "ax2.plot(n_trees_list, precs)\n",
    "ax2.set_title(\"Precision\")\n",
    "ax2.set_xlabel(\"N. trees\")\n",
    "\n",
    "ax3.plot(n_trees_list, recs)\n",
    "ax3.set_title(\"Recall\")\n",
    "ax3.set_xlabel(\"N. trees\")\n",
    "\n",
    "ax4.plot(n_trees_list, f1s)\n",
    "ax4.set_title(\"F1 score\")\n",
    "ax4.set_xlabel(\"N. trees\")\n",
    "\n",
    "plt.tight_layout()  # Optional: Adjust the spacing between subplots\n",
    "\n",
    "# plt.savefig('images/rf_n_trees.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Class DTNode, DecisionTree, Bagging, RandomForest must be defined in the same space where 'load_model' is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(obj, file_path):\n",
    "    \"\"\"\n",
    "    Save an object to a file using pickle.\n",
    "\n",
    "    Parameters:\n",
    "    obj (object): The object to be saved.\n",
    "    file_path (str): The path to the file where the object will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "        print(\"Object saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the object: {str(e)}\")\n",
    "\n",
    "def load_model(file_path):\n",
    "    \"\"\"\n",
    "    Load an object from a file using pickle.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file containing the object.\n",
    "\n",
    "    Returns:\n",
    "    object: The loaded object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            obj = pickle.load(file)\n",
    "        print(\"Object loaded successfully.\")\n",
    "        return obj\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading the object: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save model\n",
    "rf_clf = RandomForest(num_trees=5, num_features=150)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "save_model(rf_clf, 'randomforest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "clf = load_model('randomforest.pkl')\n",
    "\n",
    "id = 9\n",
    "\n",
    "print('(Graduated: 1.0, Dropout: -1.0)')\n",
    "print('- Prediction :', clf.predict(X_test[id,:].reshape(1,-1))[0][0])\n",
    "print('- Actual     :', y_test[id][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
